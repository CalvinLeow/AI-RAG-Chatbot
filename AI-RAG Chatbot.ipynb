{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Dn9ZmJ2BqRm"
      },
      "source": [
        "# **AI-RAG Chatbot - Code**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "sn8YYBzSFt2P"
      },
      "outputs": [],
      "source": [
        "# Install Ollama and LangChain's Ollama integration\n",
        "!curl -fsSL https://ollama.com/install.sh | sh\n",
        "!pip install langchain_community"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "BZjgWQbzoYom"
      },
      "outputs": [],
      "source": [
        "# Start the Ollama service in the background\n",
        "import subprocess\n",
        "process = subprocess.Popen([\"ollama\", \"serve\"])\n",
        "#!ollama list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "aM9Jdhv_o2qw"
      },
      "outputs": [],
      "source": [
        "# Pull the llama3 model (as required)\n",
        "!ollama pull llama3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "wvMlbpS9HE7d"
      },
      "outputs": [],
      "source": [
        "# Get the Colab notebook URL for Ollama\n",
        "from google.colab.output import eval_js\n",
        "notebook_url = eval_js(\"google.colab.kernel.proxyPort(11434)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "r2GBSJBApADJ"
      },
      "outputs": [],
      "source": [
        "!export OLLAMA_HOST=notebook_url"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "zXFoyq4SkbKE"
      },
      "outputs": [],
      "source": [
        "# Initialize the LLM\n",
        "from langchain_community.llms import Ollama\n",
        "llm = Ollama(model=\"llama3\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "nK9l7lROHlJl"
      },
      "outputs": [],
      "source": [
        "!pip install gradio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oJHgeCJRTQiW"
      },
      "outputs": [],
      "source": [
        "# Install required libraries\n",
        "!pip install PyPDF2\n",
        "!pip install sentence-transformers\n",
        "!pip install faiss-gpu\n",
        "\n",
        "# Download the PDF\n",
        "!wget -O DataStructures_Cheatsheet.pdf \"https://www.dropbox.com/scl/fi/pelyq7ygf7uqa0ktjkwoj/DataStructures_Cheatsheet_Zero_To_Mastery_V1.01.pdf?rlkey=fztti2ursdlrm5maqo9wwjnl5&dl=0\"\n",
        "\n",
        "import PyPDF2\n",
        "import numpy as np\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7xm7pk-j9io_"
      },
      "outputs": [],
      "source": [
        "# Function to extract text from the PDF\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    with open(pdf_path, 'rb') as file:\n",
        "        reader = PyPDF2.PdfReader(file)\n",
        "        text = \"\"  # Initialize text as an empty string\n",
        "        for page in reader.pages:\n",
        "            page_text = page.extract_text()\n",
        "            if page_text:  # Check if text extraction was successful\n",
        "                text += page_text\n",
        "    return text\n",
        "\n",
        "# Function to split text into chunks\n",
        "def split_text_into_chunks(text, chunk_size=500):\n",
        "    return [text[i:i+chunk_size] for i in range(0, len(text), chunk_size)]\n",
        "\n",
        "# Load pre-trained model for embeddings\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "# Function to create embeddings\n",
        "def create_embeddings(text_chunks):\n",
        "    return model.encode(text_chunks)\n",
        "\n",
        "# Function to create FAISS index\n",
        "def create_faiss_index(embeddings):\n",
        "    dimension = embeddings.shape[1]\n",
        "    index = faiss.IndexFlatL2(dimension)  # L2 distance\n",
        "    index.add(embeddings)  # Add embeddings to the index\n",
        "    return index\n",
        "\n",
        "# Extract text from the PDF\n",
        "pdf_path = 'DataStructures_Cheatsheet.pdf'\n",
        "text = extract_text_from_pdf(pdf_path)\n",
        "\n",
        "# Create chunks and embeddings\n",
        "chunks = split_text_into_chunks(text)\n",
        "embeddings = create_embeddings(chunks)\n",
        "\n",
        "# Create FAISS index\n",
        "index = create_faiss_index(embeddings)\n",
        "\n",
        "# Function to retrieve relevant text (define this function)\n",
        "def retrieve_relevant_text(query, index, chunks):\n",
        "    # Process the query to create its embedding\n",
        "    query_embedding = model.encode([query])\n",
        "\n",
        "    # Search for the closest embedding in the FAISS index\n",
        "    distances, indices = index.search(query_embedding, k=5)  # Adjust 'k' as needed\n",
        "    relevant_chunks = [chunks[i] for i in indices[0]]\n",
        "    relevant_text = \" \".join(relevant_chunks)\n",
        "    # print(f\"Indices Retrieved: {indices}\")\n",
        "    # print(f\"Distances Retrieved: {distances}\")\n",
        "\n",
        "    # Return the relevant text chunks as a single string\n",
        "    return relevant_text\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "xz7A8DSwu6RM"
      },
      "outputs": [],
      "source": [
        "# Function to handle conversation\n",
        "conversation_history = []\n",
        "\n",
        "def chatbot_conversation(user_input):\n",
        "    # Define the base response prompt with CO-STAR framework structure\n",
        "    base_prompt = (\n",
        "        \"1. Context: You are a helpful chatbot for students learning Data Structures and Algorithms.\\n\"\n",
        "        \"2. Outcome: Answer my question in 50 words or less, and ask me a follow up question on the same topic.\\n\"\n",
        "        \"3. Scale: The chatbot will handle queries from students with varying levels of knowledge in DSA, offering tailored explanations and probing questions based on their responses.\\n\"\n",
        "        \"4. Tone: Maintain a positive and motivational tone throughout, fostering a sense of empowerment and encouragement. It should feel like a friendly guide offering valuable insights.\\n\"\n",
        "        \"5. Actor: The primary participants are the students asking questions and you, the chatbot providing answers and guidance.\\n\"\n",
        "        \"6. Resources: The chatbot leverages the llama model and the provided textbook information to generate accurate and insightful responses, along with a structured approach to maintaining context and engaging the student.\\n\"\n",
        "        \"When responding, make sure to provide an answer and then ask a relevant follow-up question. I should be allowed to change the topic without first answering your follow-up question as long it is still related to Data Structures and Algorithm.\"\n",
        "        \"If I mention anything irrelevant to Data Structures and Algorithms, decline to answer and tell me to ask a question related to DSA.\"\n",
        "    )\n",
        "\n",
        "    # Store conversation history\n",
        "    conversation_history.append(f\"User: {user_input}\")  # Store the user's input\n",
        "    context = \" \".join(conversation_history[-6:])  # Last 3 pairs (6 lines)\n",
        "\n",
        "    # Retrieve relevant text from the PDF\n",
        "    relevant_text = retrieve_relevant_text(user_input, index, chunks)\n",
        "\n",
        "    # Combine base prompt with context and user input\n",
        "    full_prompt = f\"{base_prompt} Context from previous conversation: {context} Relevant information from textbook (For RAG, use only if necessary): {relevant_text} User Input/Response: {user_input}\"\n",
        "\n",
        "    # Logging: Print the breakdown of the full prompt\n",
        "    # print(\"Breakdown of Full Prompt:\")\n",
        "    # print(\"-\" * 50)\n",
        "    # print(\"Relevant Information from Textbook:\")\n",
        "    # print(relevant_text)\n",
        "    # print(\"User Input:\")\n",
        "    # print(user_input)\n",
        "    # print(\"-\" * 50)\n",
        "\n",
        "    # Use the LLM to generate a response\n",
        "    response = llm.invoke(full_prompt)\n",
        "\n",
        "    # Append the response to the conversation history\n",
        "    conversation_history.append(f\"Chatbot: {response}\")  # Store the LLM's response\n",
        "\n",
        "    return response  # Return the generated response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m_eaH4DEvAs1"
      },
      "outputs": [],
      "source": [
        "import gradio as gr\n",
        "\n",
        "# Create the Gradio interface\n",
        "interface = gr.Interface(\n",
        "    fn=chatbot_conversation,\n",
        "    inputs=gr.Textbox(placeholder=\"Type your DSA-related question here...\", label=\"User Input\"),\n",
        "    outputs=gr.Textbox(label=\"Chatbot Response\"),\n",
        "    title=\"DSA Chatbot\",\n",
        ")\n",
        "\n",
        "# Launch the Gradio interface with debugging enabled\n",
        "interface.launch(debug=True)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "A634d8-iH3Yd"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
